{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones para top 5 por categoría\n",
    "## En función de ArrDelay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asume que los datos estan en ../data/<año>.csv.bz2\n",
    "# fixme: poner el directorio de data que esté en el repo cuando pusheen los pibes\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import fnmatch\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from cml import normal_equations_solver as nes\n",
    "\n",
    "#sns.set(color_codes=True)\n",
    "\n",
    "# load data\n",
    "tmp_dir = \"./tmp_csv/\"\n",
    "years = [str(year) for year in range(1994,2009)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmse(pred,y):\n",
    "    return math.sqrt(mean_squared_error(pred, y)) / (y.max()-y.min())\n",
    "\n",
    "max_grade = 1\n",
    "phases = [i*math.pi/2 for i in range(4)]\n",
    "monthly_freqs =  [3, 4, 6, 12]\n",
    "\n",
    "class lsqPredictor:\n",
    "    def __init__(self, phases, freqs, max_grade):\n",
    "        self.phases = phases\n",
    "        self.freqs = freqs\n",
    "        self.max_grade = max_grade\n",
    "    \n",
    "    def trig_vals(self, t):\n",
    "        return  [math.sin(2 * math.pi / f * t + p) for f in self.freqs for p in self.phases]\n",
    "\n",
    "    def get_x_vals(self, x):\n",
    "        x_vals = []\n",
    "        for i in range(len(x)):\n",
    "            row = np.array([i**p for p in range(self.max_grade+1)] + self.trig_vals(i), dtype='float')\n",
    "            x_vals.append(row)\n",
    "        return x_vals\n",
    "    \n",
    "    def print_coefs(self):\n",
    "        c = 0\n",
    "        for i in range(self.max_grade+1):\n",
    "            print(\"Coef de x^{}:\\n{}\".format(i, self.coefs[c]))\n",
    "            c += 1\n",
    "        for i in self.freqs:\n",
    "            for j in self.phases:\n",
    "                print(\"Coef de sinusoide con frecuencia {} y fase {}:\\n{}\".format(i, j, self.coefs[c]))\n",
    "                c += 1\n",
    "                \n",
    "    def fit(self, train_set):\n",
    "        arrays = self.get_x_vals(train_set)\n",
    "        A = np.stack(arrays)\n",
    "        self.coefs = nes.lstsq(A, train_set)    \n",
    "        \n",
    "    def pred(self, pred_set):\n",
    "        arrays = self.get_x_vals(pred_set)\n",
    "        full_A = np.stack(arrays)        \n",
    "        return full_A@self.coefs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguir top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top(column, yrs):    \n",
    "    df_acc = pd.DataFrame(columns=[\"FlightNum\", column]).set_index(column)\n",
    "    for year in yrs:\n",
    "        print(\"Leyendo csv: {}\".format(year))\n",
    "        df = pd.read_csv(\"../data/\"+year+\".csv.bz2\", compression=\"bz2\", \\\n",
    "                         usecols=[\"FlightNum\", column], \\\n",
    "                         encoding=\"ISO 8859-1\")    \n",
    "\n",
    "\n",
    "        df = df.groupby(by=column).count()\n",
    "        df_acc = pd.concat([df_acc, df]).groupby(column).sum()\n",
    "        del df \n",
    "    top = df_acc.nlargest(5, 'FlightNum')\n",
    "    return top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear directorio con data de delays agrupada por filter_column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grouped_files(subdir, years, subset, filter_column, delay_column=\"ArrDelay\"):\n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir)\n",
    "\n",
    "    if not os.path.exists(tmp_dir + subdir):\n",
    "        os.makedirs(tmp_dir + subdir)\n",
    "    \n",
    "    df_cut = pd.DataFrame(columns=[\"date\", filter_column, delay_column])\n",
    "    df_full = pd.DataFrame(columns=[\"date\", filter_column, delay_column])\n",
    "    for year in years:\n",
    "        print(\"Leyendo csv: {}\".format(year))\n",
    "        df = pd.read_csv(\"../data/\"+year+\".csv.bz2\", compression=\"bz2\", \\\n",
    "                         usecols=[\"Month\", \"Year\", \"DayofMonth\", \"DayOfWeek\", delay_column, filter_column], \\\n",
    "                         encoding=\"ISO 8859-1\")    \n",
    "\n",
    "        #solo los del valor pedido\n",
    "        df = df.loc[df[filter_column].isin(subset)]\n",
    "\n",
    "        #formato mas feliz para fechas\n",
    "        dates = pd.to_datetime(df.Year*10000+df.Month*100+df.DayofMonth, format='%Y%m%d')\n",
    "        df[\"date\"] = dates\n",
    "\n",
    "        #acumulamos en full antes de recortar outliers\n",
    "        df = df[[\"date\", delay_column, filter_column]]\n",
    "        dg = df.groupby(\n",
    "                [pd.Grouper(key='date', freq='M'), pd.Grouper(key=filter_column)] \n",
    "            ).mean().reset_index()\n",
    "\n",
    "        df_full = pd.concat([df_full, dg], sort=False)\n",
    "        \n",
    "        #sacamos outliers\n",
    "        low = 0.1\n",
    "        high = 0.9\n",
    "\n",
    "        group = [pd.Grouper(key='date', freq='M'), pd.Grouper(key=filter_column)]\n",
    "        df = df.groupby(group) \\\n",
    "                .apply(lambda x : \n",
    "                  x[(x[delay_column] >= x[delay_column].quantile(low)) & \n",
    "                    (x[delay_column] <= x[delay_column].quantile(high))]\n",
    "              .mean()\n",
    "              ).reset_index()\n",
    "        \n",
    "        if (year==1994):\n",
    "            print(df)\n",
    "        \n",
    "        df_cut = pd.concat([df_cut, df], sort=False)\n",
    "        del df \n",
    "\n",
    "    df_cut.to_csv(tmp_dir+subdir+\"cut\"+\".csv\")\n",
    "    df_full.to_csv(tmp_dir+subdir+\"full\"+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotear datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df1, df2, train_limit_axis, unit_str, delay_column=\"ArrDelay\"):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.xlabel('Predicción para el i-ésimo {}'.format(unit_str))\n",
    "    \n",
    "    ax1 = df1.dl_pred.plot(color='orange', grid=True, label='pred delay', legend=True)\n",
    "    df2[delay_column].plot(color='blue', grid=True, secondary_y=False, label='real delay', ax=ax1, legend=True)\n",
    "\n",
    "    xticks = ax1.xaxis.get_major_ticks()\n",
    "    plt.axvline(x=train_limit_axis, color=\"green\")\n",
    "    plt.show()\n",
    "\n",
    "#max_grade = 0    \n",
    "#Validación cruzada y gráficos\n",
    "#devuelve array de nrmse\n",
    "def cross_val_delays(subdir, filter_column, entity, n=4, delay_column=\"ArrDelay\"):\n",
    "    #for trim in [\"full\", \"cut\"]:\n",
    "    #al final anda mejor full\n",
    "    trim = \"full\"\n",
    "    \n",
    "    print(\"\\n\\n------Trim de outliers: {}------------------------------------\".format(trim))\n",
    "\n",
    "    df = pd.read_csv(tmp_dir+\"{}/{}.csv\".format(subdir, trim))\n",
    "    df = df[df[filter_column] == entity]\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df.sort_values(by=\"date\")\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n)\n",
    "    df_full = pd.read_csv(tmp_dir+\"{}/full.csv\".format(subdir))\n",
    "    df_full = df_full[df_full[filter_column] == entity]\n",
    "    df_full['date'] = pd.to_datetime(df_full['date'], errors='coerce')\n",
    "\n",
    "    #si recortamos alguna fecha por outliers la sacamos de df_full tambien para comparar\n",
    "    if trim == \"cut\":\n",
    "        df_full = df_full.loc[df_full[\"date\"].isin(df.date)]\n",
    "\n",
    "    errors = []\n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        delays_train, delays_test = df.iloc[train_index][delay_column], df.iloc[test_index][delay_column]\n",
    "        lpr = lsqPredictor(phases, monthly_freqs, max_grade)\n",
    "        lpr.fit(delays_train)\n",
    "        #lpr.print_coefs()\n",
    "        train_test_delays = df[delay_column]\n",
    "\n",
    "        df2 = df\n",
    "        day_train_limit = df2.iloc[len(delays_train)][\"date\"]\n",
    "\n",
    "        print(\"Entrena hasta la fecha: {}\".format(day_train_limit))\n",
    "        df2[\"dl_pred\"] = lpr.pred(train_test_delays) \n",
    "        plot_df(df2.reset_index(), df_full.reset_index(), len(delays_train), \"mes\", delay_column)\n",
    "\n",
    "        error = nrmse(\n",
    "            df2.loc[df[\"date\"].dt.date > day_train_limit].dl_pred, \n",
    "            df_full.loc[df_full[\"date\"].dt.date > day_train_limit][delay_column]\n",
    "        )\n",
    "\n",
    "        print(\"Error:{} \\n\\n\".format(error))\n",
    "        errors.append(error)\n",
    "    del df\n",
    "    del df2\n",
    "    del df_full\n",
    "    avg_error = np.average(errors)\n",
    "    print(\"Error promedio de {}:{} \\n\\n\".format(entity, avg_error))\n",
    "    return errors\n",
    "    \n",
    "def get_error_and_plot_top(top_rank, entity_name, subdir, filter_column, delay_column=\"ArrDelay\"):\n",
    "    errors = []\n",
    "    for entity in top_rank:\n",
    "        print(\"{}: {}\".format(entity_name.capitalize(), entity))\n",
    "        errors += cross_val_delays(subdir, filter_column, entity, n=4, delay_column=delay_column)\n",
    "    print(\"Error promedio para top 5 {}: {}\".format(entity_name, np.average(errors)))\n",
    "    return errors\n",
    "\n",
    "cat_error = pd.DataFrame(columns=[\"category\", \"error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 aerolíneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conseguimos el top\n",
    "years = [str(year) for year in range(1994,2000)]\n",
    "\n",
    "if not os.path.exists(tmp_dir+\"carriers/\"):\n",
    "    os.makedirs(tmp_dir+\"carriers/\")\n",
    "\n",
    "if not os.path.exists(tmp_dir+\"carriers/top_carriers.csv\"):\n",
    "    df = get_top(\"UniqueCarrier\", years)\n",
    "    df.to_csv(tmp_dir+\"carriers/top_carriers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(tmp_dir+\"carriers/top_carriers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_carriers = df.reset_index()[\"UniqueCarrier\"]\n",
    "top_carriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtramos los vuelos del top\n",
    "years = [str(year) for year in range(1994, 2009)]\n",
    "\n",
    "if not os.path.exists(tmp_dir+\"carriers/cut.csv\") or not os.path.exists(tmp_dir+\"carriers/full.csv\"):\n",
    "    create_grouped_files(\"carriers/\", years, top_carriers, \"UniqueCarrier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_grade = 1\n",
    "#graficamos y calculamos errores\n",
    "carrier_errors = get_error_and_plot_top(top_carriers, \"Carrier\", \"carriers/\", \"UniqueCarrier\")\n",
    "for err in carrier_errors:\n",
    "    cat_error = cat_error.append({\"category\":\"Carrier\", \"error\":err}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 aeropuertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conseguimos el top\n",
    "years = [str(year) for year in range(1994,2000)]\n",
    "\n",
    "if not os.path.exists(tmp_dir+\"airports/\"):\n",
    "    os.makedirs(tmp_dir+\"airports/\")\n",
    "\n",
    "if not os.path.exists(tmp_dir+\"airports/top_airports.csv\"):\n",
    "    df = get_top(\"Origin\", years)\n",
    "    df.to_csv(tmp_dir+\"airports/top_airports.csv\")\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(tmp_dir+\"airports/top_airports.csv\")\n",
    "top_airports = df.reset_index()[\"Origin\"]\n",
    "top_airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conseguir sus vuelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(year) for year in range(1994,2009)]\n",
    "\n",
    "if not os.path.exists(tmp_dir+\"airports/cut.csv\") or not os.path.exists(tmp_dir+\"airports/full.csv\"):\n",
    "    create_grouped_files(\"airports/\", years, top_airports, \"Origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_grade = 1\n",
    "\n",
    "#graficamos y calculamos errores\n",
    "airport_errors = get_error_and_plot_top(top_airports, \"Airport\", \"airports/\", \"Origin\")\n",
    "for err in airport_errors:\n",
    "    cat_error = cat_error.append({\"category\":\"Airport\", \"error\":err}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 rutas\n",
    "### Funciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_routes(yrs):    \n",
    "    df_acc = pd.DataFrame(columns=[\"FlightNum\", \"origin_city\", \"dest_city\"]).set_index([\"origin_city\", \"dest_city\"])\n",
    "    df_cities = pd.read_csv(\"../data/airports.csv\", usecols=[\"city\", \"iata\"])\n",
    "    origin_df = df_cities.rename(columns={\"city\": \"origin_city\", \"iata\": \"origin_iata\"})  \n",
    "    dest_df = df_cities.rename(columns={\"city\": \"dest_city\", \"iata\": \"dest_iata\"})  \n",
    "    \n",
    "    for year in yrs:\n",
    "        print(\"Leyendo csv: {}\".format(year))\n",
    "        df = pd.read_csv(\"../data/\"+year+\".csv.bz2\", compression=\"bz2\", \\\n",
    "                         usecols=[\"FlightNum\", \"Origin\", \"Dest\"], \\\n",
    "                         encoding=\"ISO 8859-1\")    \n",
    "        \n",
    "        #agrego las ciudades\n",
    "        df = df.merge(origin_df, left_on='Origin', right_on='origin_iata')\n",
    "        df = df.merge(dest_df, left_on='Dest', right_on='dest_iata')\n",
    "        \n",
    "        #ciudades que mas aparecen\n",
    "        df = df.groupby([\"origin_city\", \"dest_city\"]).count()\n",
    "        df_acc = pd.concat([df_acc, df], join=\"inner\").groupby([\"origin_city\", \"dest_city\"]).sum()\n",
    "        del df \n",
    "    top = df_acc.nlargest(5, 'FlightNum')\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conseguimos vuelos del top de pares de ciudades\n",
    "#como ahora agrupamos por tuplas de ciudades no podemos reusar el create_cities_grouped (sin romperlo antes)\n",
    "#las funciones que siguen hace lo mismo que venimos haciendo pero para pares de ciudades\n",
    "def create_cities_grouped_files(subdir, years, delay_column=\"ArrDelay\"):\n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir)\n",
    "\n",
    "    if not os.path.exists(tmp_dir + subdir):\n",
    "        os.makedirs(tmp_dir + subdir)\n",
    "    \n",
    "    df_cut = pd.DataFrame(columns=[\"date\", \"dest_city\", \"origin_city\", delay_column])\n",
    "    df_full = pd.DataFrame(columns=[\"date\", \"dest_city\", \"origin_city\", delay_column])\n",
    "    \n",
    "    df_cities = pd.read_csv(\"../data/airports.csv\", usecols=[\"city\", \"iata\"])\n",
    "    origin_df = df_cities.rename(columns={\"city\": \"origin_city\", \"iata\": \"origin_iata\"})  \n",
    "    dest_df = df_cities.rename(columns={\"city\": \"dest_city\", \"iata\": \"dest_iata\"})  \n",
    "    \n",
    "    routes = top_routes.reset_index()\n",
    "    routes = routes[[\"origin_city\", \"dest_city\"]]\n",
    "    \n",
    "    for year in years:\n",
    "        print(\"Leyendo csv: {}\".format(year))\n",
    "        df = pd.read_csv(\"../data/\"+year+\".csv.bz2\", compression=\"bz2\", \\\n",
    "                         usecols=[\"Month\", \"Year\", \"DayofMonth\", \"DayOfWeek\", delay_column, \"Origin\", \"Dest\"], \\\n",
    "                         encoding=\"ISO 8859-1\")    \n",
    "\n",
    "        #formato mas feliz para fechas\n",
    "        dates = pd.to_datetime(df.Year*10000+df.Month*100+df.DayofMonth, format='%Y%m%d')\n",
    "        df[\"date\"] = dates\n",
    "        df = df[[\"date\", delay_column, \"Origin\", \"Dest\"]]\n",
    "\n",
    "        #agrego las ciudades\n",
    "        df = df.merge(origin_df, left_on='Origin', right_on='origin_iata')\n",
    "        df = df.merge(dest_df, left_on='Dest', right_on='dest_iata')\n",
    "        df = df[[\"date\", delay_column, \"origin_city\", \"dest_city\"]]\n",
    "        \n",
    "        #solo los del valor pedido\n",
    "        df = df.merge(routes, how=\"inner\")\n",
    "        #acumulamos en full antes de recortar outliers\n",
    "        dg = df.groupby(\n",
    "                [pd.Grouper(key='date', freq='M'), pd.Grouper(key=\"origin_city\"),  pd.Grouper(key=\"dest_city\")] \n",
    "            ).mean().reset_index()\n",
    "        \n",
    "        df_full = pd.concat([df_full, dg], join=\"outer\" , sort=False)\n",
    "        \n",
    "        #sacamos outliers\n",
    "        low = 0.1\n",
    "        high = 0.9\n",
    "\n",
    "        group = [pd.Grouper(key='date', freq='M'), pd.Grouper(key=\"origin_city\"),  pd.Grouper(key=\"dest_city\")]\n",
    "        df = df.groupby(group) \\\n",
    "                .apply(lambda x : \n",
    "                  x[(x[delay_column] >= x[delay_column].quantile(low)) & \n",
    "                    (x[delay_column] <= x[delay_column].quantile(high))]\n",
    "              .mean()\n",
    "              ).reset_index()\n",
    "        \n",
    "        if (year==1994):\n",
    "            print(df)\n",
    "    \n",
    "        df_cut = pd.concat([df_cut, df], join=\"outer\" , sort=False)\n",
    "        del df \n",
    "\n",
    "    df_cut.to_csv(tmp_dir+subdir+\"cut\"+\".csv\")\n",
    "    df_full.to_csv(tmp_dir+subdir+\"full\"+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_grade = 0    \n",
    "\n",
    "#graficamos el top de pares de ciudades\n",
    "def get_error_and_plot_top_cities(top_rank, subdir, delay_column=\"ArrDelay\"):\n",
    "    top_errors = []\n",
    "    avg_errors = []\n",
    "    for [src_city, dest_city] in top_rank:\n",
    "        print(\"Cities: {}->{}\".format(src_city, dest_city))\n",
    "\n",
    "        df_full = pd.read_csv(tmp_dir+subdir+\"/full.csv\")\n",
    "        df_full = df_full[(df_full.origin_city == src_city) & (df_full.dest_city == dest_city)]\n",
    "        df_full['date'] = pd.to_datetime(df_full['date'], errors='coerce')\n",
    "        \n",
    "        #for trim in [\"cut\", \"full\"]:\n",
    "        trim = \"full\"\n",
    "        print(\"Trim: {}\".format(trim))\n",
    "        df = pd.read_csv(\"{}{}/{}.csv\".format(tmp_dir, subdir, trim))\n",
    "        df = df[(df.origin_city == src_city) & (df.dest_city == dest_city)]\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df.sort_values(by=\"date\")\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "        #si recortamos alguna fecha por outliers la sacamos de df_full tambien para comparar\n",
    "        if trim == \"cut\":\n",
    "            df_full = df_full.loc[df_full[\"date\"].isin(df.date)]\n",
    "\n",
    "        errors = []\n",
    "        for train_index, test_index in tscv.split(df):\n",
    "            delays_train, delays_test = df.iloc[train_index][delay_column], df.iloc[test_index][delay_column]\n",
    "            lpr = lsqPredictor(phases, monthly_freqs, max_grade)\n",
    "            lpr.fit(delays_train)\n",
    "            #lpr.print_coefs()\n",
    "            train_test_delays = df[delay_column]\n",
    "\n",
    "            df2 = df\n",
    "            day_train_limit = df2.iloc[len(delays_train)][\"date\"]\n",
    "\n",
    "            print(\"Entrena hasta la fecha: {}\".format(day_train_limit))\n",
    "            df2[\"dl_pred\"] = lpr.pred(train_test_delays) \n",
    "            plot_df(df2.reset_index(), df_full.reset_index(), len(delays_train), \"mes\", delay_column)\n",
    "\n",
    "            error = nrmse(\n",
    "                df2.loc[df[\"date\"].dt.date > day_train_limit].dl_pred, \n",
    "                df_full.loc[df_full[\"date\"].dt.date > day_train_limit][delay_column]\n",
    "            )\n",
    "\n",
    "            print(\"Error:{} \\n\\n\".format(error))\n",
    "            errors.append(error)\n",
    "            del df2\n",
    "        \n",
    "        top_errors += errors\n",
    "        avg_errors.append(np.average(errors))\n",
    "        print(\"Error promedio de {}->{}: {} \\n\\n\".format(src_city, dest_city, np.average(errors)))\n",
    "        del df\n",
    "        del df_full\n",
    "    print(\"Error promedio en gral. de pares de ciudades: {} \\n\\n\".format(np.average(avg_errors)))\n",
    "    return top_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conseguimos el top\n",
    "years = [str(y) for y in range(1994, 2000)]\n",
    "\n",
    "if not os.path.exists(tmp_dir+\"cities/\"):\n",
    "    os.makedirs(tmp_dir+\"cities/\")\n",
    "\n",
    "if not os.path.exists(tmp_dir+\"cities/top_cities.csv\"):\n",
    "    top_routes = get_top_routes(years)\n",
    "    top_routes.to_csv(tmp_dir+\"cities/top_cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_routes = pd.read_csv(tmp_dir+\"cities/top_cities.csv\")\n",
    "top_routes.reset_index()\n",
    "print(top_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(y) for y in range(1994, 2009)]\n",
    "\n",
    "if not os.path.exists(tmp_dir+\"cities/cut.csv\") or not os.path.exists(tmp_dir+\"cities/full.csv\"):\n",
    "    create_cities_grouped_files(\"cities/\", years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_grade = 1\n",
    "routes = top_routes.reset_index()[[\"origin_city\", \"dest_city\"]].values\n",
    "\n",
    "#graficamos y calculamos errores\n",
    "airport_errors = get_error_and_plot_top_cities(routes, \"cities/\")\n",
    "for err in airport_errors:\n",
    "    cat_error = cat_error.append({\"category\":\"Cities\", \"error\":err}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"category\", y=\"error\", data=cat_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones para top 5 por categoría\n",
    "## Ahora en función de DepDelay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_error_dep = pd.DataFrame(columns=[\"category\", \"error\"])\n",
    "tmp_dir = \"./tmp_csv/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 aerolíneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(tmp_dir+\"carriers_depdelay/\"):\n",
    "    os.makedirs(tmp_dir+\"carriers_depdelay/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(tmp_dir+\"carriers/top_carriers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_carriers = df.reset_index()[\"UniqueCarrier\"]\n",
    "top_carriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtramos los vuelos del top\n",
    "years = [str(year) for year in range(1994, 2009)]\n",
    "if  not os.path.exists(tmp_dir+\"carriers_depdelay/cut.csv\") or \\\n",
    "    not os.path.exists(tmp_dir+\"carriers_depdelay/full.csv\"):\n",
    "    create_grouped_files(\"carriers_depdelay/\", years, top_carriers, \"UniqueCarrier\", \"DepDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_grade = 1\n",
    "#graficamos y calculamos errores\n",
    "carrier_errors = get_error_and_plot_top(top_carriers, \"Carrier\", \"carriers_depdelay/\", \"UniqueCarrier\", \"DepDelay\")\n",
    "for err in carrier_errors:\n",
    "    cat_error_dep = cat_error_dep.append({\"category\":\"Carrier\", \"error\":err}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 aeropuertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(tmp_dir+\"airports_depdelay/\"):\n",
    "    os.makedirs(tmp_dir+\"airports_depdelay/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(tmp_dir+\"airports/top_airports.csv\")\n",
    "top_airports = df.reset_index()[\"Origin\"]\n",
    "top_airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conseguir sus vuelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(year) for year in range(1994,2009)]\n",
    "\n",
    "if  not os.path.exists(tmp_dir+\"airports_depdelay/cut.csv\") or \\\n",
    "    not os.path.exists(tmp_dir+\"airports_depdelay/full.csv\"):\n",
    "    \n",
    "    create_grouped_files(\"airports_depdelay/\", years, top_airports, \"Origin\", \"DepDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_grade = 1\n",
    "\n",
    "#graficamos y calculamos errores\n",
    "airport_errors = get_error_and_plot_top(top_airports, \"Airport\", \"airports_depdelay/\", \"Origin\", \"DepDelay\")\n",
    "for err in airport_errors:\n",
    "    cat_error_dep = cat_error_dep.append({\"category\":\"Airport\", \"error\":err}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 pares de ciudades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(tmp_dir+\"cities_delay/\"):\n",
    "    os.makedirs(tmp_dir+\"cities_delay/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_routes = pd.read_csv(tmp_dir+\"cities/top_cities.csv\")\n",
    "top_routes.reset_index()\n",
    "print(top_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(y) for y in range(1994, 2009)]\n",
    "\n",
    "if  not os.path.exists(tmp_dir+\"cities_delay/cut.csv\") or \\\n",
    "    not os.path.exists(tmp_dir+\"cities_delay/full.csv\"):\n",
    "\n",
    "    create_cities_grouped_files(\"cities_delay/\", years, \"DepDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_grade = 1\n",
    "routes = top_routes.reset_index()[[\"origin_city\", \"dest_city\"]].values\n",
    "\n",
    "#graficamos y calculamos errores\n",
    "airport_errors = get_error_and_plot_top_cities(routes, \"cities_delay/\", \"DepDelay\")\n",
    "for err in airport_errors:\n",
    "    cat_error_dep = cat_error_dep.append({\"category\":\"Cities\", \"error\":err}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_error_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"category\", y=\"error\", data=cat_error_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
